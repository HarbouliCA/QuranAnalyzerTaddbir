import requests
import json
import numpy as np
import pickle
import re
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ---
# Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬Ø§Ù‹ ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨ÙƒÙØ§Ø¡Ø© Ø¹Ø§Ù„ÙŠØ© Ù„Ù„ÙÙ‡Ù… Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'

def normalize_text(text):
    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠ Ù„Ù„Ø¨Ø­Ø« (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠÙÙ‡Ù… Ø§Ù„Ù…Ø¹Ù†Ù‰ØŒ Ù„ÙƒÙ†Ù‡ Ù…ÙÙŠØ¯)
    text = re.sub(r'[\u064B-\u065F\u0670\u06D6-\u06ED\u06E5\u06E6]', '', text)
    text = text.replace("Ù±", "Ø§").replace("Ø¥", "Ø§").replace("Ø£", "Ø§").replace("Ø¢", "Ø§")
    text = text.replace("Ø©", "Ù‡").replace("Ù‰", "ÙŠ")
    return text

def load_quran_data():
    print("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†...")
    url = "https://raw.githubusercontent.com/risan/quran-json/main/dist/quran.json"
    response = requests.get(url)
    data = response.json()
    formatted_data = {}
    
    for surah in data:
        surah_name = surah['name']
        verses = []
        for ayah in surah['verses']:
            # Ù†Ø­ØªÙØ¸ Ø¨Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ ÙˆØ¨Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
            clean_text = normalize_text(ayah['text'])
            verses.append({
                "number": ayah['id'],
                "text": ayah['text'],
                "clean_text": clean_text,
                "ref": f"{surah_name} ({surah['id']}:{ayah['id']})"
            })
        formatted_data[surah_name] = verses
    return formatted_data

def process_quran_vectors():
    print("ğŸ§  Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ù„Ø£ÙˆÙ„ Ù…Ø±Ø©)...")
    model = SentenceTransformer(MODEL_NAME)
    
    quran = load_quran_data()
    all_topics = []
    global_topic_id = 0
    
    print("âš™ï¸ Ø¬Ø§Ø±ÙŠ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‚Ø±Ø¢Ù† Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…ÙˆØ¶ÙˆØ¹ÙŠØ© ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª...")

    for surah_name, verses in quran.items():
        # ØªØ¬Ù…ÙŠØ¹ Ù…Ø¨Ø¯Ø¦ÙŠ Ù„Ù„Ø¢ÙŠØ§Øª (Ù…Ø«Ù„Ø§Ù‹ ÙƒÙ„ 3-5 Ø¢ÙŠØ§Øª ØªØ´ÙƒÙ„ ÙˆØ­Ø¯Ø© Ø³ÙŠØ§Ù‚ÙŠØ© ØµØºÙŠØ±Ø©)
        # Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù„Ø¯Ù…Ø¬ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©
        
        current_chunk_verses = []
        current_chunk_text = ""
        
        for i, ayah in enumerate(verses):
            current_chunk_verses.append(ayah)
            current_chunk_text += " " + ayah['clean_text']
            
            # Ù…Ù†Ø·Ù‚ Ø§Ù„ØªÙ‚Ø³ÙŠÙ…: Ù†ØºÙ„Ù‚ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¥Ø°Ø§ ÙˆØµÙ„ Ø­Ø¬Ù…Ø§Ù‹ Ù…Ø¹ÙŠÙ†Ø§Ù‹ Ø£Ùˆ (ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ±Ù‡ Ù„ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø§Ø¨Ù‡)
            # Ù‡Ù†Ø§ Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ø§ÙØ°Ø© Ø§Ù†Ø²Ù„Ø§Ù‚ÙŠØ© Ø°ÙƒÙŠØ© Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚
            
            # Ø¥Ø°Ø§ ÙˆØµÙ„Ù†Ø§ Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø³ÙˆØ±Ø© Ø£Ùˆ ØªØ¬Ù…Ø¹Øª Ù„Ø¯ÙŠÙ†Ø§ 5 Ø¢ÙŠØ§Øª (ÙƒÙ…ØªÙˆØ³Ø· Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù‚ØµÙŠØ±)
            if len(current_chunk_verses) >= 5 or i == len(verses) - 1:
                global_topic_id += 1
                
                # *** Ø§Ù„Ø³Ø­Ø± Ù‡Ù†Ø§: Ù†Ø­Ø³Ø¨ "Ù…ØªØ¬Ù‡ Ø§Ù„Ù…Ø¹Ù†Ù‰" Ù„Ù‡Ø°Ø§ Ø§Ù„Ù…Ù‚Ø·Ø¹ ÙƒØ§Ù…Ù„Ø§Ù‹ ***
                topic_vector = model.encode(current_chunk_text.strip())
                
                all_topics.append({
                    "id": global_topic_id,
                    "surah": surah_name,
                    "verses": current_chunk_verses,
                    "full_text": current_chunk_text.strip(), # Ù„Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„
                    "vector": topic_vector # Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ù„Ù„Ù…Ø¹Ù†Ù‰
                })
                
                # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ù‚Ø§Ø¯Ù…
                current_chunk_verses = []
                current_chunk_text = ""

    # ÙØµÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø§Ù„Ù†ØµÙŠØ©) Ø¹Ù† (Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©) Ù„Ù„Ø­ÙØ¸
    json_data = []
    vectors_data = []
    
    for t in all_topics:
        json_data.append({
            "id": t['id'],
            "surah": t['surah'],
            "verses": t['verses'],
            "full_text": t['full_text']
        })
        vectors_data.append(t['vector'])

    print(f"ğŸ’¾ Ø­ÙØ¸ {len(json_data)} Ù…ÙˆØ¶ÙˆØ¹Ø§Ù‹...")
    
    # 1. Ø­ÙØ¸ Ø§Ù„Ù†ØµÙˆØµ Ù„Ù„Ø¹Ø±Ø¶
    with open("quran_topic_graph.json", "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
        
    # 2. Ø­ÙØ¸ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ
    with open("topic_embeddings.pkl", "wb") as f:
        pickle.dump(np.array(vectors_data), f)
        
    print("âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø£Ø·Ù„Ø³ Ø§Ù„Ø°ÙƒÙŠ Ø¨Ù†Ø¬Ø§Ø­!")

if __name__ == "__main__":
    process_quran_vectors()