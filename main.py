# main.py
import json
import numpy as np
from data_loader import load_mock_quran
from ai_engine import Semantics
from sequential_processor import StreamProcessor
from global_unifier import unify_global_topics

# Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø­ÙØ¸ Ø§Ù„Ù€ Numpy Arrays ÙÙŠ JSON
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyEncoder, self).default(obj)

def run_pipeline():
    # 1. Ø§Ù„ØªØ¬Ù‡ÙŠØ²
    quran_data = load_mock_quran()
    ai = Semantics()
    processor = StreamProcessor(ai_engine=ai, threshold=0.60) 
    # Threshold 0.60 Ø¬ÙŠØ¯ Ù„Ù„ØªÙØ±ÙŠÙ‚ Ø¨ÙŠÙ† Ø§Ù„Ù‚ØµØµ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø¯Ø§Ø®Ù„ Ø§Ù„Ø³ÙˆØ±Ø©

    all_local_topics = []

    # 2. Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ù„Ù…Ø³Ø­ Ø§Ù„ØªØªØ§Ø¨Ø¹ÙŠ (Sequential Scan)
    print("ğŸš€ Starting Sequential Analysis...")
    for surah_name, verses in quran_data.items():
        print(f"   Analyzing: {surah_name}...")
        topics = processor.process_surah(surah_name, verses)
        all_local_topics.extend(topics)

    # 3. Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ (Global Unification)
    final_graph = unify_global_topics(all_local_topics, ai, merge_threshold=0.70)

    # 4. Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    output = []
    for theme in final_graph:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø­ÙØ¸ (Ø­Ø°Ù Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù)
        clean_theme = {
            "theme_id": theme["id"],
            "related_segments": theme["occurrences"]
        }
        output.append(clean_theme)

    with open("quran_topic_graph.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=4, cls=NumpyEncoder)

    print(f"\nâœ… Done! Identified {len(output)} unique global themes.")
    print("ğŸ“‚ Results saved to 'quran_topic_graph.json'")

if __name__ == "__main__":
    run_pipeline()